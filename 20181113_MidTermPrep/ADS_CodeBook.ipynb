{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules we need to use for this session\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "# PCA module\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "import seaborn as sbn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful Commands\n"
     ]
    }
   ],
   "source": [
    "'merge with suffixes'\n",
    "# Pdata=LEHDR.merge(LEHDW, on=None, left_index=True, right_index=True, how='inner', suffixes=('','W'))\n",
    "\n",
    "'rename a praticular column of dataframe'\n",
    "# Pdata.rename(columns={'total':'totalR'},inplace=True)\n",
    "\n",
    "'Plot scatter plot from dataframe'\n",
    "# Pdata.plot(x='totalR',y='totalW',kind='scatter',title='Working population against residential')\n",
    "\n",
    "'Plot with log scales'\n",
    "# Pdata.plot(x='totalR',y='totalW',kind='scatter',title='Working population against residential',logx=True,logy=True)\n",
    "\n",
    "'For calculating log: if possibility is that any number can be 0'\n",
    "# x_ = math.log( x_+(x_==0))\n",
    "\n",
    "'Finding the row corresponding to maximum of a column'\n",
    "# REStaten_.loc[REStaten_['SALE_PRICE'].idxmax()]\n",
    "\n",
    "'in case csv doesnot load properly'\n",
    "# file = open(filename, errors='ignore')\n",
    "# df = pd.read_csv(file)\n",
    "\n",
    "'For scaling axes'\n",
    "# from sklearn import preprocessing\n",
    "# scaler = preprocessing.StandardScaler().fit(X)\n",
    "# XS=scaler.transform(X) \n",
    "\n",
    "'For PCA'\n",
    "# pca = PCA(No.of PCA required,whiten=True) #PCA model, paramerized by the target number of components or target percentage of variance\n",
    "# pca.fit(XS) #fit the model to the data\n",
    "# P_=pca.transform(XS) #transform the data to principal components\n",
    "\n",
    "'Pca explained variance'\n",
    "# pca.explained_variance_\n",
    "\n",
    "print(\"Useful Commands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute correlation coefficients between multiple features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Compute correlation coefficcients between multiple features'\n",
    "# df[[columns]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to map values as heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-7-642080bfe65b>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-642080bfe65b>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    plt.show()\u001b[0m\n\u001b[0m              \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "# to read geopnadas shapefile\n",
    "'''\n",
    "import geopandas as gpd\n",
    "tzs = gpd.read_file('taxi_zones/taxi_zones.shp')\n",
    "\n",
    "# Function for heatmap\n",
    "def plot_on_map1(geodf,df,column,left_on,right_on,mergeAndDo=True,areaNorm=False,logScale=False):\n",
    "    # Check whether to merge or not\n",
    "    if mergeAndDo == True:\n",
    "        newdf = geodf.merge(df, left_on=left_on, right_on=right_on, how='inner')\n",
    "    else:\n",
    "        newdf = geodf\n",
    "        \n",
    "    # Assign the value to visualize\n",
    "    newdf['Visual'] = newdf[column]\n",
    "    \n",
    "    # Check whether to do area normalization or not\n",
    "    if areaNorm:\n",
    "        newdf['Visual'] = newdf['Visual'] / newdf['Shape_Area'].astype(float)\n",
    "    \n",
    "    #c Check whether to do log scaling or not\n",
    "    if logScale:\n",
    "        import math\n",
    "        newdf['Visual'] = newdf['Visual'].apply( lambda x: math.log(x+(x==0)))\n",
    "    \n",
    "    # Plot it\n",
    "    fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "    ax = newdf.plot(column = 'Visual', ax = ax, cmap = 'magma', legend = True)\n",
    "    ax.axis('scaled')\n",
    "    plt.title((column+\" heatmap\"), fontsize = 20) #add title\n",
    "    ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimate bi-variate and multi-variate linear regressions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bivariate regression without intercept\n",
    "'''\n",
    "import statsmodels.formula.api as smf\n",
    "lm = smf.ols(formula='SALE_PRICE~GROSS_SQUARE_FEET-1', data = REStaten_sf).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bivariate regression with intercept\n",
    "'''\n",
    "import statsmodels.formula.api as smf\n",
    "lm = smf.ols(formula='SALE_PRICE~GROSS_SQUARE_FEET', data = REStaten_sf).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with regression lines\n",
    "'''\n",
    "REStaten_sf.plot(kind='scatter',x='GROSS_SQUARE_FEET',y='SALE_PRICE')\n",
    "plt.plot(REStaten_sf.GROSS_SQUARE_FEET,REStaten_sf.predicted_price,'r-', label = 'Without Intercept')\n",
    "plt.plot(REStaten_sf.GROSS_SQUARE_FEET,REStaten_sf.predicted_price2,'y-', label = 'With Intercept')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate regression\n",
    "'''\n",
    "formula='y~x+'+'+'.join(['x%d'%p for p in range(2,M+1)]); formula\n",
    "lm = smf.ols(formula=formula, data = data1).fit()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make formula for multivariate regression when we have a list of predictors\n",
    "'''\n",
    "output= 'Y'\n",
    "formula = output +' ~ '+ '+'.join(['x0','x1','x2','x3','x4'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform out-of-sample evaluation and cross-validation for the linear models;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform random train and test split\n",
    "'''\n",
    "np.random.seed(2016)\n",
    "Shuffle = np.random.permutation(len(data3))\n",
    "train = data3.take(Shuffle)[:30]\n",
    "test = data3.take(Shuffle)[30:]\n",
    "'''\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.loc[:,lis_predictor], data3.loc[output], test_size = 0.4, random_state = 200)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For displaying out of sample R^2\n",
    "'''\n",
    "def modelEval(lm, key = 'Y', df_test):\n",
    "    lmy = lm.predict(df_test)\n",
    "    y_err = lmy - df_test[key]\n",
    "    y_norm = df_test[key]-np.mean(df_test[key])\n",
    "    return 1-y_err.dot(y_err)/y_norm.dot(y_norm)  # R2 w.r.t. test set\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS performance of OLS when we subsequently add variables; Number of regressor with highest OS R2\n",
    "'''\n",
    "df_train =\n",
    "df_test =\n",
    "output=\n",
    "\n",
    "lis_predictor=df_train.columns\n",
    "lis_predictor.remove(output)\n",
    "\n",
    "Number_variables=range(len(lis_predictor))\n",
    "OLS_R_2_OS_F=[]\n",
    "OLS_R_2_IS_F=[]\n",
    "\n",
    "def modelEval(lm, key = 'Y', df_test):\n",
    "    lmy = lm.predict(df_test)\n",
    "    y_err = lmy - df_test[key]\n",
    "    y_norm = df_test[key]-np.mean(df_test[key])\n",
    "    return 1-y_err.dot(y_err)/y_norm.dot(y_norm)  # R2 w.r.t. test set\n",
    "\n",
    "for i in Number_variables:\n",
    "    \n",
    "    lm = smf.ols(formula = 'Y ~ '+ '+'.join(lis_predictor[:i+1]), data = df_train).fit()\n",
    "    R2 = modelEval(lm, key = output, df_test  )\n",
    "    OLS_R_2_IS_F.append(lm.rsquared)\n",
    "    OLS_R_2_OS_F.append(R2 if R2 > 0 else 0)\n",
    "\n",
    "print(\"Number of regressors with maximum OS R2: \",np.argmax(OLS_R_2_OS_F)+1)\n",
    "plt.title('OS performance of OLS when we subsequently add variables')\n",
    "plt.plot(Number_variables,OLS_R_2_OS_F,'b',label='R_squared_OLS_OS')\n",
    "plt.plot(Number_variables,OLS_R_2_IS_F,'g',label='R_squared_OLS_IS')\n",
    "plt.legend(loc='lower left')\n",
    "plt.xlabel('Number of independent variables')\n",
    "plt.ylabel('R-squared')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation given: X_train, y_train, N =range(total Number of regressors), no. of cv_folds(cv_folds)\n",
    "\n",
    "# define function to obtain optimum number of principal components for best Out of smaple R2\n",
    "# by performing 20 fold cross validation\n",
    "'''\n",
    "def pca_r2(data,Y,N, cv_folds, Graph=True):\n",
    "    best_R2 = -1\n",
    "    best_N = -1\n",
    "    R_IS_Overall=[]\n",
    "    R_OS_Overall=[]\n",
    "    for npca in N:\n",
    "        np.random.seed(0)\n",
    "        # Apply cross-validation to diagnose model for overfitting\n",
    "        R_IS=[]; R_OS=[] #lists for recording in-sample (training) and out-of-sample (test) R2\n",
    "        n = cv_folds #number of runs\n",
    "        for i in range(n):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:],Y,test_size=0.3) #randomly split data into training (70%) and test (30%)\n",
    "            #now apply PCA to the training data\n",
    "            pca = PCA(npca) #take only N leading principal components to get enough data but avoid overfitting\n",
    "            P_train=pca.fit_transform(X_train)\n",
    "            res=smf.OLS(np.array(y_train),sm.add_constant(P_train)).fit()\n",
    "            #and apply the same rotation transform we learned for training sample to the test sample\n",
    "            #this is important that we learn PCA transform from the training sample only rather than from both - training and test\n",
    "            #as this is part of the principle component regression we need to train; and all training should happen over the training set\n",
    "            P_test=pca.transform(X_test)\n",
    "            y_pred=np.array(res.predict(sm.add_constant(P_test)))\n",
    "            #print(y_pred)\n",
    "            #print(y_test)\n",
    "            R_IS.append(res.rsquared)                                                                     \n",
    "            err=y_pred-np.array(y_test)\n",
    "            R_OS.append(1-((y_pred-np.array(y_test))**2).sum()/((np.array(y_test)-np.mean(np.array(y_test)))**2).sum())    \n",
    "        R_IS_Overall.append(np.mean(R_IS))\n",
    "        R_OS_Overall.append(np.mean(R_OS))\n",
    "        if np.mean(R_OS) > best_R2:\n",
    "            best_R2 = np.mean(R_OS)\n",
    "            best_N = npca\n",
    "    print(\"The optimum number of Principal Component are: \", best_N)\n",
    "    print(\"The best OS-R-squared is: \", best_R2)\n",
    "    \n",
    "    if Graph==True:\n",
    "        plt.title('OS-R-squared for different N')\n",
    "        plt.xlabel('Number of Principal Component')\n",
    "        plt.plot(N,R_OS_Overall,'b',label='20 Fold CV_Score')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('R-squared')\n",
    "        plt.axvline(best_N,color='r',linestyle='--')\n",
    "\n",
    "        plt.show()\n",
    "'''        \n",
    "# pca_r2(data,Y,list(range(1,31)), 20, Graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regularize linear models using Rigle and Lasso;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a ridge regression given: X_train, y_train, X_test, y_test  \n",
    "'''\n",
    "from sklearn import linear_model\n",
    "\n",
    "Ridge=linear_model.Ridge(fit_intercept=True, alpha=1) #try Ridge with an arbitrary regularization parameter alpha=1\n",
    "\n",
    "Ridge.fit(X_train,y_train)\n",
    "\n",
    "# In the sample:\n",
    "p_IS=Ridge.predict(X_train)\n",
    "err_IS=p_IS-y_train\n",
    "R_2_IS_Ridge = 1-np.var(err_IS)/np.var(y_train)\n",
    "print(\"The R-squared we found for IS Ridge is: {0}\".format(R_2_IS_Ridge))\n",
    "\n",
    "Ridge_coef=Ridge.coef_\n",
    "\n",
    "#Out of sample\n",
    "p_OS=Ridge.predict(X_test)\n",
    "err_OS=p_OS-y_test\n",
    "R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
    "print(\"The R-squared we found for OS Ridge is: {0}\".format(R_2_OS_Ridge))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a lasso regression given: X_train, y_train, X_test, y_test\n",
    "'''\n",
    "Lasso=linear_model.Lasso(fit_intercept=True,alpha=1)\n",
    "#try Lasso with an arbirtary regularization parameter alpha\n",
    "\n",
    "Lasso.fit(X_train,y_train)\n",
    "# In the sample:\n",
    "p_IS=Lasso.predict(X_train)\n",
    "err_IS=p_IS-y_train\n",
    "R_2_IS_Lasso=1-np.var(err_IS)/np.var(y_train)\n",
    "print(\"The R-squared we found for IS Lasso is: {0}\".format(R_2_IS_Ridge))\n",
    "\n",
    "Lasso_coef=Lasso.coef_\n",
    "#Out of sample\n",
    "p_OS=Lasso.predict(X_test)\n",
    "err_OS=p_OS-y_test\n",
    "R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test)\n",
    "print(\"The R-squared we found for OS Lasso is: {0}\".format(R_2_OS_Lasso))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal alpha for ridge/lasso given: X_train, y_train\n",
    "\n",
    "'''\n",
    "# Find the Alpha and report best test performance for Ridge/Lasso.\n",
    "def Regularization_fit_alpha(model,X_train,y_train,alphas,p=0.4,Graph=False, logl=False):\n",
    "    #model = 1-Ridge, 2-Lasso\n",
    "    #lambdas: a list of lambda values to try\n",
    "    #p: ratio of the validation sample size / total training size\n",
    "    #Graph: plot the graph of R^2 values for different lambda\n",
    "\n",
    "    R_2_OS=[]\n",
    "    X_train0, X_valid, y_train0, y_valid = train_test_split(X_train,\n",
    "                                    y_train, test_size = 0.4, random_state = 200)\n",
    "\n",
    "    if model==1:\n",
    "        RM = lambda a: linear_model.Ridge(fit_intercept=True, alpha=a)\n",
    "        model_label='Ridge'\n",
    "    else:\n",
    "        RM = lambda a: linear_model.Lasso(fit_intercept=True, alpha=a)\n",
    "        model_label='Lasso'\n",
    "    \n",
    "    best_R2 = -1\n",
    "    best_alpha = alphas[0]\n",
    "    \n",
    "    for i in alphas:\n",
    "        lm = RM(i)\n",
    "        lm.fit(X_train0,y_train0)  #fit the regularization model\n",
    "        y_predict=lm.predict(X_valid) #compute the prediction for the validation sample \n",
    "        err_OS=y_predict-y_valid\n",
    "        R_2_OS_=1-np.var(err_OS)/np.var(y_valid)\n",
    "        R_2_OS.append(R_2_OS_)\n",
    "        if R_2_OS_ > best_R2:\n",
    "            best_R2 = R_2_OS_\n",
    "            best_alpha = i\n",
    "    \n",
    "    if Graph==True:\n",
    "        plt.title('OS-R-squared for different Alpha')\n",
    "        if logl:\n",
    "            plt.xlabel('ln(Alpha)')\n",
    "            l=np.log(alphas)\n",
    "            bl=np.log(best_alpha)\n",
    "        else:\n",
    "            plt.xlabel('Alpha')\n",
    "            l=alphas\n",
    "            bl=best_alpha\n",
    "        plt.plot(l,R_2_OS,'b',label=model_label)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('R-squared')\n",
    "        plt.axvline(bl,color='r',linestyle='--')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return best_alpha\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal alpha for ridge/lasso given: X_train, y_train, X_valid, y_valid\n",
    "'''\n",
    "alphas=np.exp(np.linspace(-10,5,200))\n",
    "\n",
    "# define function for finding optimal alpha\n",
    "def Regularization_fit_alpha(X_train,y_train,X_valid, y_valid, alphas,p=0.4,Graph=False, logl=False):\n",
    "    \n",
    "    RM = lambda a: linear_model.Lasso(fit_intercept=True, alpha=a)\n",
    "    model_label='Lasso'\n",
    "    best_R2 = -1\n",
    "    best_alpha = alphas[0]\n",
    "    R_2_OS=[]\n",
    "    \n",
    "    for i in alphas:\n",
    "        lm = RM(i)\n",
    "        lm.fit(X_train,y_train)  #fit the regularization model\n",
    "        y_predict=lm.predict(X_valid) #compute the prediction for the validation sample \n",
    "        err_OS=y_predict-y_valid\n",
    "        R_2_OS_=1-np.var(err_OS)/np.var(y_valid)\n",
    "        R_2_OS.append(R_2_OS_)\n",
    "        if R_2_OS_ > best_R2:\n",
    "            best_R2 = R_2_OS_\n",
    "            best_alpha = i\n",
    "    \n",
    "    if Graph==True:\n",
    "        plt.title('OS-R-squared for different Alpha')\n",
    "        if logl:\n",
    "            plt.xlabel('ln(Alpha)')\n",
    "            l=np.log(alphas)\n",
    "            bl=np.log(best_alpha)\n",
    "        else:\n",
    "            plt.xlabel('Alpha')\n",
    "            l=alphas\n",
    "            bl=best_alpha\n",
    "        plt.plot(l,R_2_OS,'b',label=model_label)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('R-squared')\n",
    "        plt.axvline(bl,color='r',linestyle='--')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return best_alpha\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the fit of the regression;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the fit\n",
    "'''\n",
    "data = pd.DataFrame()\n",
    "data = data.append(X_train)\n",
    "data = data.append(X_valid)\n",
    "data = data.append(X_test)\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y = y.append(y_train)\n",
    "y = y.append(y_valid)\n",
    "y = y.append(y_test)\n",
    " \n",
    "data['y_lasso']=Lasso.predict(data[lis_predictors])\n",
    "data['y_ridge']=ridge.predict(data[lis_predictors])\n",
    "\n",
    "plt.plot(data.x,y,'.r') #plot the data\n",
    "plt.plot(data.x,data.y_lasso,'b',label='Lasso') #plot the regression\n",
    "plt.plot(data.x,data.y_ridge,'g',label='Lasso')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "#plt.ylim(700,2000)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the performance of linear, lasso and ridge regression over IS and OS given: X_train, y_train, X_test, y_test\n",
    "\n",
    "'''\n",
    "OLS_R_2_OS_F=[]\n",
    "OLS_R_2_IS_F=[]\n",
    "OLS_R_2_Ridge_OS_F=[]\n",
    "OLS_R_2_Ridge_IS_F=[]\n",
    "OLS_R_2_Lasso_OS_F=[]\n",
    "OLS_R_2_Lasso_IS_F=[]\n",
    "\n",
    "Ridge=linear_model.Ridge(fit_intercept=True,alpha=alpha_r_optimal)\n",
    "Lasso=linear_model.Lasso(fit_intercept=True, alpha=alpha_l_optimal)\n",
    "\n",
    "Number_variables = range(len(X_train.columns))\n",
    "\n",
    "for j in Number_variables:\n",
    "    # OLS\n",
    "    lm = smf.ols(formula = 'Y ~ '+ '+'.join(data3.columns[:j+1]), \n",
    "                 data = pd.concat([X_train.ix[:,:j+1],y_train], axis = 1)).fit()\n",
    "    error = lm.predict(X_test.ix[:,:j+1]) - y_test\n",
    "    R_2_OS_OLS=1-error.var()/y_test.var()\n",
    "    R_2_IS_OLS = lm.rsquared\n",
    "    OLS_R_2_IS_F.append(R_2_IS_OLS)\n",
    "    OLS_R_2_OS_F.append(max(R_2_OS_OLS,0))\n",
    "    \n",
    "    # Ridge\n",
    "    Ridge.fit(X_train.ix[:,:j+1],y_train)\n",
    "    \n",
    "    # In sample:\n",
    "    err_IS=Ridge.predict(X_train.ix[:,:j+1]) - y_train\n",
    "    R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)\n",
    "    OLS_R_2_Ridge_IS_F.append(R_2_IS_Ridge)\n",
    "    \n",
    "    #Out of sample\n",
    "    err_OS=Ridge.predict(X_test.ix[:,:j+1]) - y_test\n",
    "    R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)\n",
    "    OLS_R_2_Ridge_OS_F.append(max(R_2_OS_Ridge,0))\n",
    "\n",
    "    # Lasso\n",
    "    \n",
    "    Lasso.fit(X_train.ix[:,0:j+1],y_train)\n",
    "    \n",
    "    #In sample:\n",
    "    p_IS=Lasso.predict(X_train.ix[:,0:j+1])\n",
    "    err_IS=p_IS-y_train\n",
    "    R_2_IS_Lasso=1-np.var(err_IS)/np.var(y_train)\n",
    "    OLS_R_2_Lasso_IS_F.append(R_2_IS_Lasso)\n",
    "\n",
    "    #Out of sample\n",
    "    p_OS=Lasso.predict(X_test.ix[:,0:j+1])\n",
    "    err_OS=p_OS-y_test\n",
    "    R_2_OS_Lasso=1-np.var(err_OS)/np.var(y_test)\n",
    "    OLS_R_2_Lasso_OS_F.append(max(R_2_OS_Lasso,0))\n",
    "\n",
    "plt.title('OS performance of OLS when we subsequently add variables')\n",
    "\n",
    "plt.plot(Number_variables,OLS_R_2_IS_F,'g',label='OLS_IS')\n",
    "plt.plot(Number_variables,OLS_R_2_Lasso_IS_F,'y',label='Lasso_IS')\n",
    "plt.plot(Number_variables,OLS_R_2_Ridge_IS_F,'k',label='Ridge_IS')\n",
    "\n",
    "plt.plot(Number_variables,OLS_R_2_OS_F,'b',label='OLS_OS')\n",
    "plt.plot(Number_variables,OLS_R_2_Lasso_OS_F,'c',label='Lasso_OS')\n",
    "plt.plot(Number_variables,OLS_R_2_Ridge_OS_F,'r',label='Ridge_OS')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Number of independent variables')\n",
    "plt.ylabel('R-squared')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize the multi-dimensional data in the two-dimensional space of its leading principal components;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PCA\n",
    "'''\n",
    "pca = PCA(No.of PCA required,whiten=True) #PCA model, paramerized by the target number of components or target percentage of variance\n",
    "pca.fit(XS) #fit the model to the data\n",
    "P_=pca.transform(XS) #transform the data to principal components\n",
    "'''\n",
    "\n",
    "# For Pca explained variance\n",
    "'''\n",
    "pca.explained_variance_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot explained variance per number of leading PC's to choose\n",
    "'''\n",
    "eigenvalues = pca.explained_variance_\n",
    "plt.bar(np.arange(n), eigenvalues.cumsum()/eigenvalues.sum(),align='center');\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,64)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('variance')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Subset data with required rows and columns\n",
    "zillow_new=zillow.loc[zillow['City']=='New York',['City', 'State', 'Metro', 'CountyName', 'SizeRank'] + list(zillow)[-60:]]\n",
    "\n",
    "# drop na values\n",
    "zillow_new = zillow_new.dropna()\n",
    "\n",
    "# normalize rows by last 60 months average\n",
    "zillow_new[list(zillow_new)[-60:]]=zillow_new[list(zillow_new)[-60:]].div(zillow_new[list(zillow_new)[-60:]].mean(axis=1), axis=0) #normalize activity of various cathegories within zip code by total\n",
    "\n",
    "# Standardize month columns\n",
    "for c in list(zillow_new)[-60:]:\n",
    "    zillow_new[c]=zillow_new[c]-zillow_new[c].mean()/zillow_new[c].std()\n",
    "\n",
    "# Apply PCA and obtain first 2 principal components    \n",
    "pca = PCA(len(list(zillow_new)[-60:]))\n",
    "P_train=pca.fit_transform(zillow_new[list(zillow_new)[-60:]])\n",
    "zillow_new['P1']=P_train[:,0]\n",
    "zillow_new['P2']=P_train[:,1]\n",
    "\n",
    "# Plot first 2 principal component and color as per borough name\n",
    "groups =zillow_new.groupby('CountyName')\n",
    "fig, ax = plt.subplots()\n",
    "ax.margins(0.01) # Optional, just adds 5% padding to the autoscaling\n",
    "for name, group in groups:\n",
    "    ax.plot(group.P1, group.P2, marker='o', linestyle='', ms=5, label=name)\n",
    "ax.legend()\n",
    "plt.xlabel('P1')\n",
    "plt.ylabel('P2')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply k-means, Gausian mixture and DBScan clustering;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Mean clustering: Given : scaled factors mean=0 and deviation=1\n",
    "'''\n",
    "#now do the same with KMeans from sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "#initialize the model\n",
    "km=KMeans(random_state=1,n_clusters=2) #look for two clusters; \n",
    "#k-means generally depends on random initial locations of the centroids, \n",
    "#so setting random state needed for result stability (although not really this time)\n",
    "res=km.fit(X) #train the model\n",
    "y_t=res.labels_\n",
    "print(y_t) #output the result\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gausssian clustering\n",
    "'''\n",
    "from sklearn.mixture import GaussianMixture # You can import this only if you are using 0.18+ sklearn.\n",
    "gm=GaussianMixture(n_components=n,random_state=324)\n",
    "res1=gm.fit(X)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBscan clustering\n",
    "'''\n",
    "from sklearn.cluster import DBSCAN\n",
    "res2 = DBSCAN(eps=0.33, min_samples=4).fit(X)\n",
    "res2.labels_\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build spatial visualization of the clusters of spatial regions using their shapefile;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom matplotlib.colors import LinearSegmentedColormap\\ncolors=[\\'green\\',\\'blue\\',\\'red\\',\\'yellow\\',\\'magenta\\']\\n\\n\\ndef plot_on_map(gdf, labels, mergeColumnGdf, mergeColumnLabels, valueColumnLabels): # gdf: shapefile, column: which can be used as ID to map with labels, labels: ID and label\\n    \\n    # Define colors to be used\\n    from matplotlib.colors import LinearSegmentedColormap\\n    colors=[\\'green\\',\\'blue\\',\\'red\\',\\'yellow\\',\\'magenta\\']\\n    \\n    # Merge gdf with labels\\n    new_gdf = gdf.merge(labels, left_on= , right_on= ,how = \"inner\")\\n    \\n    #visualize zip codes using clusters for picking colors        \\n    f, ax = plt.subplots(1, figsize=(12, 12))     \\n    for c in range(max(labels[valueColumnLabels]())+1): #for each cluster\\n        new_gdf.loc[new_gdf[valueColumnLabels]==c].plot(axes=ax, color=colors[c]) #visualize zip codes which belong to it using cluster color\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add a function visualizing shapes by cluster\n",
    "'''\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors=['green','blue','red','yellow','magenta']\n",
    "\n",
    "\n",
    "def plot_on_map(gdf, labels, mergeColumnGdf, mergeColumnLabels, valueColumnLabels): # gdf: shapefile, column: which can be used as ID to map with labels, labels: ID and label\n",
    "    \n",
    "    # Define colors to be used\n",
    "    from matplotlib.colors import LinearSegmentedColormap\n",
    "    colors=['green','blue','red','yellow','magenta']\n",
    "    \n",
    "    # Merge gdf with labels\n",
    "    new_gdf = gdf.merge(labels, left_on= , right_on= ,how = \"inner\")\n",
    "    \n",
    "    #visualize zip codes using clusters for picking colors        \n",
    "    f, ax = plt.subplots(1, figsize=(12, 12))     \n",
    "    for c in range(max(labels[valueColumnLabels]())+1): #for each cluster\n",
    "        new_gdf.loc[new_gdf[valueColumnLabels]==c].plot(axes=ax, color=colors[c]) #visualize zip codes which belong to it using cluster color\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find optimal number of clusters using Silhuette and Elbow methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhoutte for kmean clustering\n",
    "'''\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "def silhouetteScore(X):\n",
    "    silhouette=[]\n",
    "    range_n_clusters = range(2,len(x-1))\n",
    "    for n_clusters in range_n_clusters: #try different numbers of clusters\n",
    "        km = KMeans(n_clusters=n_clusters, random_state=324)\n",
    "        cluster_labels = km.fit_predict(X)\n",
    "        #report average Silhouette score\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        silhouette.append(silhouette_avg)\n",
    "        print(\"For n_clusters ={},\".format(n_clusters)+\" the average silhouette_score is :{}\".format(silhouette_avg))\n",
    "    plt.plot(range_n_clusters,silhouette,'-')\n",
    "    return silhouette\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhoutte plot for individual clusters and cutoff\n",
    "'''\n",
    "def silhouette_score_plot(data,range_n_clusters):\n",
    "    X=data\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(10, 5)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=324)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors)\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1],\n",
    "                    marker='o', c=\"white\", alpha=1, s=200)\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the elbow curve for kmean clustering\n",
    "'''\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def elbow(data,K):\n",
    "#data is your input as numpy form\n",
    "#K is a list of number of clusters you would like to show.\n",
    "    # Run the KMeans model and save all the results for each number of clusters\n",
    "    KM = [KMeans(n_clusters=k).fit(data) for k in K]\n",
    "    \n",
    "    # Save the centroids for each model with a increasing k\n",
    "    centroids = [k.cluster_centers_ for k in KM]\n",
    "\n",
    "    # For each k, get the distance between the data with each center. \n",
    "    D_k = [cdist(data, cent, 'euclidean') for cent in centroids]\n",
    "    \n",
    "    # But we only need the distance to the nearest centroid since we only calculate dist(x,ci) for its own cluster.\n",
    "    globals()['dist'] = [np.min(D,axis=1) for D in D_k]\n",
    "    \n",
    "    # Calculate the Average SSE.\n",
    "    avgWithinSS = [sum(d)/data.shape[0] for d in dist]\n",
    "    \n",
    "    \n",
    "    # elbow curve\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, avgWithinSS, 'b*-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Average within-cluster sum of squares')\n",
    "    plt.title('Elbow for KMeans clustering')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Total with-in sum of square plot. Another way to show the result.\n",
    "    wcss = [sum(d**2) for d in dist]\n",
    "    tss = sum(pdist(data)**2)/data.shape[0]\n",
    "    bss = tss-wcss\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(K, bss/tss*100, 'b*-')\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Percentage of variance explained')\n",
    "    plt.title('Elbow for KMeans clustering')\n",
    "    plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
