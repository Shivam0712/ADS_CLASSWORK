{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation\n",
    "\n",
    "Recall the concept of Pearson correlation coefficient. Given two random variables $X,Y$ with observations $X_1,X_2,...,X_n$, $Y_1,Y_2,...,Y_n$ it is defined as\n",
    "$$\n",
    "C=\\frac{E[(X-E[X])(Y-E[Y])]}{\\sigma(X)\\sigma(Y)},\n",
    "$$\n",
    "where $E[X]=\\sum_i X_i/n$ is the mean, $\\sigma(X)=\\sqrt{E[(X-E[X])^2]}=\\sqrt{\\frac{\\sum_i(X_i-E[X])^2}{n}}$ is the standard deviation.\n",
    "\n",
    "<span style=\"color:red\">**Correlaton gets higher when $X$ and $Y$ get above/below average together and gets lower when they appear in a couterphase. One can see that $-1\\leq C\\leq 1$, where $\\pm 1$ is reached only is case of a strict linear dependence. Zero correlation is smth you would averagely expect at random.**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni-variate linear regression\n",
    "\n",
    "## Least square estimate\n",
    "One of the basic models to try, while learning dependence between two real-valued variables $X$ and $Y$ is a uni-variate linear regression:\n",
    "$$\n",
    "y=w x.\n",
    "$$\n",
    "\n",
    "Admitting that the model's accuracy is not be absolute: \n",
    "$$\n",
    "y=w x + \\varepsilon,\\hspace{5ex} (1)\n",
    "$$\n",
    "where $\\varepsilon$ is the model's error (often called noise or residual).\n",
    "\n",
    "The objective is to learn $w$ given a training set of $\\{(x_i, y_i), i=1..N\\}$ providing the best possible fit for the model. The fit could be characterized by the errors of the model\n",
    "$$\n",
    "\\varepsilon_i=y_i-w x_i\n",
    "$$\n",
    "and in particular by their sum of squares:\n",
    "$$\n",
    "RSS(w)=\\sum\\limits_i \\varepsilon_i^2=\\sum\\limits_i (y_i-w x_i)^2. \\hspace{5ex} (2)\n",
    "$$\n",
    "Minimizing such $RSS(w)\\to min$ is called a least square approach. With respect to this criteria, regression becomes an optimization problem\n",
    "$$\n",
    "\\hat{w}=argmin_w RSS(w). \\hspace{5ex} (3)\n",
    "$$\n",
    "Luckily this problem has a precise analytic solution following from RSS function's shape and local extremum criteria:\n",
    "$$\n",
    "RSS'(w)=0,\n",
    "$$\n",
    "i.e.\n",
    "$$\n",
    "0=\\frac{d\\sum\\limits_i (y_i-w x_i)^2}{dw}=\\sum\\limits_i\\frac{d (y_i-w x_i)^2}{dw}=\\sum\\limits_i -2 x_i(y_i-w x_i),\n",
    "$$$$\n",
    "0=\\sum\\limits_i x_i y_i- w \\sum\\limits_i (x_i)^2,\n",
    "$$$$\n",
    "\\sum\\limits_i x_i y_i = w \\sum\\limits_i (x_i)^2,\n",
    "$$$$\n",
    "w=\\frac{\\sum\\limits_i x_i y_i}{\\sum\\limits_i (x_i)^2}.\n",
    "$$\n",
    "Thinking of $x_i$ and $y_i$ as the observations of random variables $X$ and $Y$ this can be rewritten as\n",
    "$$\n",
    "w=\\frac{E[XY]}{E[X^2]}.\n",
    "$$\n",
    "In case variables $x,y$ were normalized as\n",
    "$$\n",
    "x:=\\frac{x-E[X]}{\\sigma[X]},\\ y:=\\frac{y-E[Y]}{\\sigma[Y]}\n",
    "$$\n",
    "in order to ensure zero averages $E[X]=E[Y]=0$ and unit standard deviations $\\sigma[X]=\\sigma[Y]=1$, the equation for $w$ could be rewritten as\n",
    "$$\n",
    "w=\\frac{E[XY]-E[X]E[Y]}{E[X^2]-E[X]^2}=\\frac{E[XY]-E[X]E[Y]}{\\sigma[X]^2}=\\frac{E[XY]-E[X]E[Y]}{\\sigma[X]\\sigma[Y]}=corr(X,Y).\n",
    "$$\n",
    "So in case of normalized $x,y$ a best fit univariate linear regression is provided by an equation\n",
    "$$\n",
    "y=corr(X,Y)x\n",
    "$$\n",
    "\n",
    "<span style=\"color:red\">**Bivariate regression coefficient is equal to correlation if both the variables are standardized**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-variate linear regression\n",
    "\n",
    "## 1. Definition and parameter estimation\n",
    "\n",
    "As we remember linear model between single real-value regressor $x$ and output variable $y$ is expressed by \n",
    "$$\n",
    "y=w_1 x + w_0 +\\varepsilon.\n",
    "$$\n",
    "$w_1$-slope coefficient, $w_0$ - intercept, $\\varepsilon$ - random noise. In a more general case when $x$ is a real-valued $n \\times 1$-vector $x=(x_1,x_2,...,x_n)^T$, the model could be easily generalized as\n",
    "$$\n",
    "y=\\sum\\limits_j w_j x_j +\\varepsilon\n",
    "$$\n",
    "or in a vector form\n",
    "$$\n",
    "y=w^T x+\\varepsilon, \\hspace{5ex} (1)\n",
    "$$\n",
    "where $w=(w_1,w_2,...,w_n)$ is also a $n \\times 1$-vector. \n",
    "Notice that intercept is not specifically separated as it could be always introduced by adding a dummy variable $x^m\\equiv 1$.\n",
    "\n",
    "The probabilistic view on the model in the assumption that $\\varepsilon\\sim {\\cal N}(0,\\sigma^2)$ is\n",
    "$$\n",
    "p(y|x,w,\\sigma)={\\cal N}(y|w^T x,\\sigma^2).\n",
    "$$\n",
    "\n",
    "Given a training set $X=\\{(x_j^i), j=1..n, i=1..N\\}$, $Y=\\{(y^i), i=1..N\\}$ (further also denote columns of $X$ as $X_j=\\{(x_j^i), i=1..N\\}$), the least square optimization criteria for inferring a vector of coefficients $w$ can be written as\n",
    "\n",
    "$$\n",
    "RSS(w)= \\sum \\limits_i \\varepsilon_i^2= \\sum \\limits_i (y^i-w^T x^i)^2. \\hspace{5ex} (2)\n",
    "$$\n",
    "\n",
    "or in a matrix form:\n",
    "$$\n",
    "RSS(w)=(Y-X w)^T(Y-X w).\n",
    "$$\n",
    "Then finding an estimate\n",
    "$$\n",
    "\\hat{w}=argmin_w RSS(w)\n",
    "$$\n",
    "can be done by solving the system (in a matrix form)\n",
    "$$\n",
    "0=\\frac{\\partial RSS(\\hat{w})}{\\partial w}=2X^T (Y-X \\hat{w}).\n",
    "$$\n",
    "Using matrix formalism the solution could be expressed as\n",
    "$$\n",
    "\\hat{w}=\\left(X^T X\\right )^{-1}X^T Y. \\hspace{5ex} (3)\n",
    "$$\n",
    "<span style=\"color:red\">** This assumes that $X^T X$ is non-singular. Otherwise we have a case of multicollinearity.**</span>\n",
    "\n",
    "## 2. Regression performance: R-squared\n",
    "\n",
    "In the same way as for the bi-variate regression:\n",
    "$$\n",
    "R^2=1-\\frac{RSS}{\\sum\\limits_i (y_i-\\overline{y})^2}=\\frac{\\sum\\limits_i (\\hat{y}_i-\\overline{y})^2}{\\sum\\limits_i (y_i-\\overline{y})^2},\n",
    "$$\n",
    "where $\\overline{y}=\\sum\\limits_i y_i$ is the sample mean of observed values of responce variable, while $\\hat{y}_i=w^T x_i$ is the predicted value of output variable as compared to the observed value of $y_i$ corresponding to $x=x_i$.\n",
    "<span style=\"color:red\">** This way $R^2$ is often interpreted as a fraction of responce variable's variance explained by linear model. $R^2=1$ is equivalent to $RSS=0$, i.e. the model fits the observations exactly, i.e. responce variable depends linearly on the explaining variables. On the other hand, $R^2=0$ means that the model always predicts the sample mean $\\overline{y}$, i.e. explaining variables $x$ have no effect on responce variable $y$.** </span>\n",
    "\n",
    "Least-square criteria is equivalent to maximizing $R^2$.\n",
    "\n",
    "In case of a bi-variate regression with intercept $R^2=corr(Y,X)^2$, i.e. is a square of a correlation coefficient between observations of input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty of regression estimates\n",
    "\n",
    "While `coef` is what we care about in the first place (mobile activity per capita), output reports multiple other quantities, such as standard error and confidence interval for this estimate.\n",
    "\n",
    "We'll get to there exact meaning later, but for now just a rough idea: regression is a probabilistic model, admitting uncertainty in both - the data itself as well as the resulting model estimates. The data could be noisy, or represent just a partial available sample of observations or account for other parameters, not directly observed (i.e. market share of the considered mobile operators or special event happening in certain areas might affect mobile activity pushing it above or below a reasonable model expectation, making such observations the outliers, introducing noise and uncertainty).\n",
    "\n",
    "But as we do not know which observations are the outliers, what if we run the regression multiple times on different samples of observations? Resulting values for the slope coefficient will form a certain distribution, as one can see below. This distribution will have not only the mean value, which will be consistent with the regression estimate reported above, but also **standard error** and **confidence intervals** (e.g. the interval where 95% most probable values belong) could be defined for it. \n",
    "\n",
    "<span style=\"color:red\"> The **p-value** is roughly speaking - our degree of confidence that zero is the right value for the slope coefficient, i.e. that the given regressor is not essential for the model. It is defined by measuring the overall probability weight (or chance of occurance) of all values of the slope coefficient being no more likely than zero, according the probability distribution we obtain for them.</span>\n",
    "\n",
    "This experiment is implemented below. By no means it explains the classic way of assessing uncertainty in regression estimates (and actually if you change the sampling fraction you'll see how it affects the standard deviation), but gives an idea of where such uncertainty might come from. We'll get to the rigorous definitions later over the course of the class, for now it is just important to understand the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic approach to the linear regression\n",
    "\n",
    "When fitting a linear model one should admit that the model's accuracy is not absolute: \n",
    "$$\n",
    "y=w_1 x + w_0 +\\varepsilon,\\hspace{5ex} (1)\n",
    "$$\n",
    "where $\\varepsilon$ is the model's error (often called noise or residual) or \n",
    "$$\n",
    "y=w^T x +\\varepsilon,\\hspace{5ex} (2)\n",
    "$$\n",
    "for the multivariate case with $x$ being an $n$-dimentional vector.\n",
    "\n",
    "Uncertainty of the model (1) prediction can be expressed by considering $\\varepsilon$ as a random variable representing deviations of the actual values of $y$ from the estimates provided by the precise linear model. A basic assumption is to have $\\varepsilon\\sim{\\cal N}(0,\\sigma^2)$ (mean is zero as otherwise the intercept term $w_0$ could be corrected accordingly). \n",
    "\n",
    "Often observations $y_i,x_i$ are spread in time ($i$ - could be time of the observation). \n",
    "Classical regression assumptions (Gauss-Markov): $\\sigma$ is the same for all observations (does not depend on time) and errors $\\varepsilon_i$ following ${\\cal N}(0,\\sigma^2)$ are mutually uncorrelated, i.e. $cov(\\epsilon_i,\\epsilon_j)=0$ for $i\\neq j$. \n",
    "\n",
    "An alternative way of representing this linear model is by considering $y$ as a random variable and expressing it's conditional probability density function with respect to given $x$ and $w$ as\n",
    "$$\n",
    "p(y|x,w)={\\cal N}(y|w_1 x+w_0,\\sigma^2).\\hspace{5ex} (3)\n",
    "$$\n",
    "\n",
    "This way it is actually the mean of the distribution (3) which is predicted by a linear model.\n",
    "Now the problem is to fit unknown parameters of the model $w_1,w_0,\\sigma$ (the last one $\\sigma$ represents how usually close are the values of $y$ to the linear prediction), so that the model is the most consistent with the training data $\\{(x_j,y_j), j=1..N\\}$.\n",
    "\n",
    "Fitting could be done through maximizing the likelihood that the observed data $y_j$ actually follows the suggested distribution (3). Likelihood is defined as a product of probability density values $\\prod\\limits_j p(y_j|x_j)$. But it's actually more practical to maximize the log-likelihood (which is equivalent):\n",
    "$$\n",
    "\\log\\left(\\prod\\limits_j p(y_j|x_j)\\right)=\\sum\\limits_j \\log\\left({\\cal N}(y|w_1 x+w_0,\\sigma^2)\\right)=\n",
    "$$$$\n",
    "=-\\sum\\limits_j \\frac{(y_j-w_1 x_j+w_0)^2}{2\\sigma^2}- N \\log(\\sigma)-N \\log(\\sqrt{2\\pi})\\to\\max\n",
    "$$\n",
    "Optimization with respect to $w$ turns out to be equivalent to the above least-square minimization. Then for minimal $RSS(\\hat{w})$ acheived, $\\sigma$ could be found from\n",
    "$$\n",
    "\\frac{RSS(\\hat{w})}{2\\sigma^2}+N\\log(\\sigma)\\to\\min\n",
    "$$$$\n",
    "\\frac{\\partial\\frac{RSS(\\hat{w})}{2\\sigma^2}+N\\log(\\sigma)}{\\partial \\sigma}=0,\n",
    "$$$$\n",
    "-\\frac{RSS(\\hat{w})}{\\sigma^3}+\\frac{N}{\\sigma}=0,\n",
    "$$$$\n",
    "\\sigma^2=\\frac{RSS(\\hat{w})}{N}.\n",
    "$$\n",
    "However often in order to account for the actual number of degrees of freedom in the model which is $N-2$ rather than $N$, the adjusted version of the above estimate is being used:\n",
    "$$\n",
    "\\sigma^2=\\frac{RSS(\\hat{w})}{N-2}\n",
    "$$\n",
    "\n",
    "In case of the multi-variate regression the probabilistic view on the model in the assumption that $\\varepsilon\\sim {\\cal N}(0,\\sigma^2)$ is\n",
    "$$\n",
    "p(y|x,w,\\sigma)={\\cal N}(y|w^T x,\\sigma^2)\\hspace{5ex} (4).\n",
    "$$\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Given a training set $X=\\{(x_j^i), j=1...n, i=1...N\\}$, $Y=\\{(y^i), i=1...N\\}$ (further also denote columns of $X$ as $X_j=\\{(x_j^i), i=1...N\\}$), the max-likelihood criteria will take form:\n",
    "$$\n",
    "\\prod\\limits_i p(y^i|x^i,w,\\sigma)\\to \\max.\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\log\\left(\\prod\\limits_i p(y^i|x^i,w,\\sigma)\\right)=\\sum\\limits_i \\log\\left({\\cal N}(y^i|w^T x^i,\\sigma^2)\\right)=\n",
    "$$$$\n",
    "=-\\sum\\limits_i \\frac{(y^i-w^T x^i)^2}{2\\sigma^2}- N \\log(\\sigma)-N \\log(\\sqrt{2\\pi})=-\\frac{RSS(w)}{2\\sigma^2}-N \\log(\\sigma)-N \\to\\max\n",
    "$$\n",
    "Which is in turn equivalent to\n",
    "$$\n",
    "RSS(w)\\to \\min,\n",
    "$$\n",
    "i.e. least-square criteria plus (just like in case of the bi-variate regression)\n",
    "$$\n",
    "\\frac{RSS(\\hat{w})}{2\\sigma^2}+N\\log(\\sigma)\\to\\min,\n",
    "$$\n",
    "i.e. the estimate $\\hat{\\sigma}$ could be found as\n",
    "$$\n",
    "\\frac{\\partial\\left[\\frac{RSS(\\hat{w})}{2\\sigma^2}+N\\log(\\hat{\\sigma})\\right]}{\\partial \\hat{\\sigma}}=0,\n",
    "$$$$\n",
    "-\\frac{RSS(\\hat{w})}{\\hat{\\sigma}^3}+\\frac{N}{\\hat{\\sigma}}=0,\n",
    "$$$$\n",
    "\\hat{\\sigma}^2=\\frac{RSS(\\hat{w})}{N}. \\hspace{5ex} (5)\n",
    "$$\n",
    "\n",
    "Also an unbiased adjusted estimate is known to be\n",
    "$$\n",
    "\\hat{\\sigma}^2=\\frac{RSS(\\hat{w})}{N-n}. \\hspace{5ex} (5')\n",
    "$$\n",
    "\n",
    "</span>\n",
    "\n",
    "### Confidence intervals\n",
    "Uncertainty of the model predictions comes together with the uncertainty of the estimate for the model's coefficients estimates $w$. If we start from uncertainty of $y$ taking the observations for the output variable as normally distributed random variables, this leads to a multinomial normal distribution\n",
    "$$\n",
    "w\\sim{\\cal N}(\\hat{w},\\sigma^2(X^T X)^{-1})\n",
    "$$\n",
    "with an average $E[w]=\\hat{w}$ and a variance-covariance matrix \n",
    "$$\n",
    "Var(w)=\\sigma^2(X^T X)^{-1}.\n",
    "$$\n",
    "However while using an estimate (5') for $\\sigma=\\hat{\\sigma}=\\sqrt{\\frac{RSS(\\hat{w})}{N-n}}$ we also have to admit its uncertainty rather than having $\\sigma$ fixed which does not allow to consider a normal distribution for $w$ anymore. Instead one can have a Student's $t$-distributions with $N-n$ degrees of freedom for the standardized quantities\n",
    "$$\n",
    "z_j=\\frac{w_j-\\hat{w_j}}{\\hat{\\sigma} \\sqrt{h_j}},\n",
    "$$\n",
    "where $h_j$ are the $j$-the diagonal elements of the matrix $(X^T X)^{-1}$.\n",
    "\n",
    "This way the confidence intervals for each $\\hat{w}_j$ given the confidence level $1-\\alpha$ are constructred as\n",
    "$$\n",
    "P\\left(w_j\\in[\\hat{w}_j-t_{\\alpha/2}\\sigma\\sqrt{h_j},\\hat{w}_j+t_{\\alpha/2}\\sigma\\sqrt{h_j}]\\right)=1-\\alpha\n",
    "$$\n",
    "where $t_{\\alpha/2}$ are quantiles of the $t$-distribution, i.e. such values that\n",
    "$$\n",
    "P(|z_j|\\leq t_{\\alpha/2})=1-\\alpha\n",
    "$$\n",
    "\n",
    "\n",
    "### Hypothesis testing: P-values, t-statistics\n",
    "\n",
    "Considering the statistical significance of each regressor $w_j$ we evaluate the hypothesis\n",
    "$$\n",
    "H_1:w_j\\neq w_j^0\n",
    "$$\n",
    "vs a null-hypothesis\n",
    "$$\n",
    "H_0:w_j=w_j^0.\n",
    "$$\n",
    "When $w_j^0=0$ this means that we are trying to validate the statistical significance of the non-zero impact of the considered regressor $w_j$. For that purpose we consider a $t$-statistics\n",
    "$$\n",
    "t=\\frac{w_j-w_j^0}{\\hat{\\sigma} \\sqrt{h_j}}.\n",
    "$$\n",
    "If $|t|>t_{\\alpha/2}$ this means that the hypothesis $H_0$ is too unlikely and should be rejected with the confidence level $1-\\alpha$.\n",
    "\n",
    "A $p$-value on the other hand is virtually speaking giving us such an $\\alpha$ based on the observed $t$, i.e. expressed the probability\n",
    "$$\n",
    "P(|z|>|t|)\n",
    "$$\n",
    "of having a $z$ randomly produced by the distribution corresponding to the null-hypothesis value $w_j=w_j^0$ with even higher $|z|$ compared to the observed $|t|$. This can be interpreted as a likelihood that the observed value of $t$ is got just by chance given $w_j=w_j^0$. If this likelihood is lower than a certain threshold value (usually $5\\%$) this is interpreted as having an equation $w_j=w_j^0$ too unlikely, i.e. having to reject the hypothesis $H_0$. Otherwise, if $p-value$ is higher than a threshold value ($>5\\%$) this means that the null-hypothesis can not be rejected with sufficient confidence (of at least $95\\%$) so this might be the case that the regressor does not have a significant impact. \n",
    "\n",
    "It is important to understand that $p$-value is not supposed to prove any specific estimate for the coefficient $w_j$ - it can only make us confident (or uncertain on the contrary) that $w_j\\neq 0$ (or different from another value of interest). Also having a low $p$-values do not prove that the impact of $x_i$ on $y$ is actually linear. <span style=\"color:red\">In turn, having high $p$-value by itself does not prove that $y$ is independent on $x_i$ (first of all - a linear impact could still be there despite being seen as unlikely, just disguised by noise; second - the actual impact could be non-linear, i.e. not subject to being described by a linear model; third - an impact of the considered regressor might be affected by an overlap with the impact of the other regressors, so it might not be impactful after others are considered, but it might still be impactful by itself.\n",
    "\n",
    "Regressors having high $p$-values are usually not included into the final model in order to stay free from the random insignificnt effects, reducing the risk of overfitting. This is however not the only way of fighting it. Also sequentually excluding regressors based on their $p$-values is not necessarily the best way of the feature selection as high $p$-values might result from multicollinearity, i.e. from having an impact of the considered regressor overlap with the impact of others. And it might happen that excluding other regressors in the first place will better help with fighting the overfitting and also might lead to the decrease of the $p$-values of the remaining regressors as a result.\n",
    "\n",
    "### Hypothesis testing: F-statistics\n",
    "\n",
    "<span style=\"color:red\">However often we need to test the significance of a subgroup of regressors together (or all of the regressors) testing the entire model against the null-hypothesis that a model based on a smaller subset of $m$ regressors (or just the intercept) of the regressors actually performs not worse than the original one.</span>  The following $F$-statistics helps with that:\n",
    "$$\n",
    "F=\\frac{(RSS_0-RSS_1)(N-n)}{RSS_1\\cdot (n-m)},\n",
    "$$\n",
    "where $RSS_1$ is the optimal (least-square) $RSS$ value for the original regression and $RSS_0$ - is the optimal $RSS$ value of the regression based on the considered subset of $m$ regressors (testing the hypothesis that only those $m$ regressors are significant). <span style=\"color:red\"> If we only consider a cumulative impact of all the regressors vs a trivial regression having just the intercept, then $m-1$ and $F$-statitics could be expressed through $R^2$:\n",
    "$$\n",
    "F=\\frac{R^2 (N-n)}{(1-R^2)(n-1)}.\n",
    "$$\n",
    "R and python will report the value of $F$ for the null-hypothesis that only the intercept is significant, as well as the $p$-value characterizing this $F$-statistics.\n",
    "<span style=\"color:red\">\n",
    "If $F$-statistics above is a critical value corresponding to a certain level of confidence $1-\\alpha$ (or if the corresponding $p$-value is below the threshold $\\alpha$ one rejects the null-hypothesis of having all the considered regressors together insignificant. Otherwise this hypothesis can not be rejected, so it might be the case that regressors do not really have an impact. But once again $F$-statistics can not prove any specific estimates for the coefficients, nor can it prove a linear impact of those regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urban scaling laws\n",
    "\n",
    "According to G.West and colleagues [Bettencourt, L.M., Lobo, J., Helbing, D., Kühnert, C. and West, G.B., 2007. Growth, innovation, scaling, and the pace of life in cities. Proceedings of the national academy of sciences, 104(17), pp.7301-7306.] many urban quantities scale superlinearly or underlinearly with city size.\n",
    "<span style=\"color:red\">\n",
    "This means they follow the law \n",
    "$$\n",
    "Y\\sim C *{\\rm Population}^q\n",
    "$$ \n",
    "with $q>1$ (for productivity metrics) or $q<1$ (for infrastructural metrics). \n",
    "\n",
    "Meaning that e.g. once the city becomes twice bigger it becomes more than twice wealthier, more innovative etc. And it takes less than twice the amount of infrastructure!\n",
    "</span>\n",
    "Below we will establish a scaling relationship between total income/crime vs population for US cities. \n",
    "\n",
    "And although scaling relationship is non-linear, linear regression can perfectly help. Just take log-scale:\n",
    "$$\n",
    "log(Y)\\sim q log(Population)+log(C)\n",
    "$$\n",
    "and think of it as a bi-variate linear regression of $log(Y)$ vs $log(Population)$ with a slope coefficient $q$ and intercept $log(C)$.\n",
    "<span style=\"color:red\">\n",
    "We will perform this regression asking a question if $q$ is substantially different from $1$ or if $q=1$ could be the right model? \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "In general a multi-variate regression model (not necessarily linear) could be expressed as looking for a functional dependence \n",
    "$$\n",
    "y=f(x)\n",
    "$$\n",
    "between an output variable $y$ and the multi-dimensional vector of input variables (features, regressors) $x=(x_1,x_2,x_3,....,x_n)$.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "When the dimensionality $n$ of the feature space (number of regressors) is too high with respect to the number of observations, it may cause multiple issues:\n",
    "\n",
    "- <span style=\"color:red\"> complexity: the model involving multiple regressors becomes difficult to fit and interpret;\n",
    "\n",
    "- <span style=\"color:red\"> multicollinearity: a large set of regressors could encounter substantial correlations, leading to multicollinearity of regressors and high variance in their estimates, making coefficients hard to interpret/rely on;\n",
    "\n",
    "- <span style=\"color:red\"> overfitting: as multiple regressors might contain a lot of relevant but also irrelevant information, the model could pick it up, becoming too specifically adjusted to the training set, which would reduce its generalizeability (performance over the validation/test set);\n",
    "\n",
    "In addition, exploratory analysis, including visualization of multi-dimensional data is much more difficult. Imagine we want to visualize complaining profiles of NYC neighborhoods? As each profile is characterized with a 162-dimensional vector, how to perform such a visualization? Would not it be more convenient to find a 2D or 3D visual representation instead?\n",
    "\n",
    "Reducing complexity of the model and its data (number of features or their dimensionality) is usually recommended in such cases. The most streighforward way of doing so is through feature selection. \n",
    "\n",
    "<span style=\"color:red\">\n",
    "Feature selection aims to reduce dimensionality of $x$ by removing some of its components $x_j$ which turn out to be the least relevant for the model, i.e. have the lowest positive or even negative impact on the model performance over the external validation set. This way feature selection provides a mapping of vectors $x$ into a shorter vector of its subcomponents, e.g.\n",
    "$$\n",
    "(x_1,x_2,x_3,x_4,x_5)\\to (x_1,x_3,x_5).\n",
    "$$\n",
    "\n",
    "But feature selection is often too rigid - we have to make our choices of keeping or getting rid of each variable entirely, though it might be the case that each regressor by itself still contains certain valuable information, but all together the feature space is redundant. As an alternative one can think of expressing the useful information contained in a large inital amount of regressors through some smaller amount of latent variables (different from the initial regressors) able to explain all or almost all the relevant information. This is called dimensionality reduction.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "In general, dimensionality reduction looks for an arbitrary mapping of the feature space (possibly transforming the features not just removing some) into a new feature space of a smaller dimension:\n",
    "$$\n",
    "x=(x_1,x_2,x_3....,x_n)\\to x'=(x_1',x_2',x_3',....,x_m),\n",
    "$$\n",
    "so that a simpler model $y=f(x')$ could be learned instead.\n",
    "\n",
    "E.g. a mapping\n",
    "$$\n",
    "(x_1,x_2,x_3,x_4,x_5)\\to x'=(x_1+x_2+x_3+x_4+x_5,x_1 x_2 x_3 x_4 x_5)\n",
    "$$\n",
    "could serve as an example of reducing the dimensionality of the feature space from $5$ to $2$.\n",
    "\n",
    "Such reduction often turnes out to be pretty useful as learning a model over a smaller set of features could be easier and such models might be more resilient to overfitting and easier to interpret. Often dimensionality could be substantially reduced without reducing the capacity of the model to fit the output variable much. This is becasue the Pareto rule is perfectly applicable in this scenario - 80% (or even 90, 95, 99%) of the useful information contained in the original regressors could be effectively packed into a much smaller amount of the new regressors (under a suitable transformation), often being 20% (or even 10, 5, 1%) of the original size. This way 80% (or even 90, 95, 99%) of the efficieny could be acheived with 20% (or even 10, 5, 1%) of the effort (complexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle component analysis\n",
    "\n",
    "A most common linear approach to dimentionality reduction is the principal component analysis. An idea is that given $N$ observations for the $n$ regressors $x_j$\n",
    "\n",
    "$$\n",
    "X=\\{x_i^j, i=1..n, j=1..N\\}\n",
    "$$\n",
    "\n",
    "we try to come up with a linear transformation\n",
    "\n",
    "$$\n",
    "U=X V,\n",
    "$$\n",
    "\n",
    "where $V$ is $nxp$-dimensional transformation matrix and $U$ is a $N x p$ matrix of new factors $u_1,u_2,...,u_p$ (columns of $U$), such that they explain as much of initial information contained in $X$ as possible for the $p$ latent variables - linear combinations of the original ones.\n",
    "\n",
    "## Technique\n",
    "<span style=\"color:red\">\n",
    "Before applying PCA variables $x_i$ are usually centered ($E[x_i]=0$) and sometimes also normalized ($var[x_i]=1$).\n",
    "\n",
    "<span style=\"color:red\">\n",
    "**Selecting first principle component**: look for the column unit basis $N x 1$ vector of weights/loadings $v_1$, such that resulting variable $u_1=X v_1$ has maximal possible variance $var[u_1]$ (captures as much information/distinctions from the complete feature space $X$):\n",
    "$$\n",
    "v_1=argmax_{v_1: v_1^T v_1=1}var[u_1] =argmax_{v_1: v_1^T v_1=1}u_1^T u_1=argmax_{v_1: v_1^T v_1=1}v_1^T X^T X v_1.\n",
    "$$\n",
    "\n",
    "Then after first basis vector $v_1$ and principle component $u_1$ are selected, the second basis vector $v_2$ is selected following the same principle, but with an additional constrain of being orthogonal to $v_1$. \n",
    "\n",
    "Similarly each following $i$-th vector $v_i$ is being defined as\n",
    "$$\n",
    "v_i=argmax_{v_i: v_i^T v_i=1, v_i^T v_j=0, j<i}v_i^T X^T X v_i.\n",
    "$$\n",
    "\n",
    "The problem is easy to solve knowing the eigenvectors of $X^T X$, i.e. such unit vectors $v_i$ ($v_i^T v_i=1$) that\n",
    "\n",
    "$$\n",
    "\\lambda_i v_i=X^T X v_i\n",
    "$$\n",
    "\n",
    "or in the matrix form\n",
    "\n",
    "$$\n",
    "diag(\\lambda)V=X^T X V\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ are the corresponding eigenvalues (mutually distinctive). First of all its easy to see that such vectors $v_i$ are always mutually orthogonal, as\n",
    "\n",
    "$$\n",
    "v_j^T v_i= v_j^T X^T X v_i/\\lambda_i =(X^T X w_j)^T v_i/\\lambda_i=v_j^T v_i \\lambda_j/\\lambda_i.\n",
    "$$\n",
    "\n",
    "This is why $V^T V=I_n$.\n",
    "\n",
    "Now if we know $n$ unit eigenvectors with distinct eigenvalues such that $\\lambda_1>\\lambda_2> ...>\\lambda_n>0$, they produce an orthogonal basis in the $n$-dimensional space and for any unit vector $w=e_1 v_1+e_2 v_2+... +e_n v_n$ we can see that\n",
    "\n",
    "$$\n",
    "w^T X^T X w=\\lambda_1 e_1^2+\\lambda_2 e_2^2+...+\\lambda_n e_n^2,\n",
    "$$\n",
    "\n",
    "which is maximized for $e_1=1, e_2=e_3=...e_n=0$ (as $\\sum_i e_i^2=1$), i.e. $w=v_1$. \n",
    "\n",
    "Similarly the second, third, etc loading vectors could be found as $v_2,v_3,...$ correspondingly.\n",
    "\n",
    "Also the solution follows from the singular value decomposition of the matrix $X$:\n",
    "\n",
    "$$\n",
    "X=W \\Sigma V^T,\n",
    "$$\n",
    "\n",
    "where $W$ is a $N x n$ matrix of mutually orthogonal unit columns, $V$ is a $n x n$ matrix of mutually orthogonal unit columns and $\\Sigma$ is an $n x n$- diagonal matrix, i.e.\n",
    "\n",
    "$$\n",
    "W^T W=V^T V=I_n\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "X^T X=V\\Sigma W^T W \\Sigma V^T=V\\Sigma^2 V^T\n",
    "$$\n",
    "\n",
    "and its easy to see that $V$ is the matrix of eigenvectors and $\\Sigma$ is the diagonal matrix of square roots of eigenvalues. Then \n",
    "\n",
    "$$\n",
    "U=XV=W\\Sigma V^T V=W\\Sigma.\n",
    "$$\n",
    "\n",
    "## PCA as a dimensionality reduction tool\n",
    "\n",
    "Now once matrixes $V, U, \\Sigma$ are defined as well as the eigenvalues $\\lambda_j$ (being the squares of the diagonal elements of $\\Sigma$), how do we use that for dimensionality reduction? \n",
    "\n",
    "Matrix $V$ performs a transformation of the regressors $x_i$ to the new orthogonal variables $u_i$ being the columns of $U$. And \n",
    "$$\n",
    "Var[u_i]=\\lambda_i\n",
    "$$\n",
    "\n",
    "This is interpreted as each variable $u_i$ containing the fraction $\\lambda_i/\\sum\\limits_j \\lambda_j$ of the entire information (variation) contained in all the regressors. And the choice of regressors $u_i$ is such that $\\lambda_1,\\lambda_2,...$ are sequentially maximized. So if one wants to select as few latent variables as possible in order to cover a given franction $\\alpha$ (often $95\\%$) of information (variation) from it is enough to select first $k$ principle components $u_1,u_2,...u_k$, so that \n",
    "\n",
    "$$\n",
    "\\frac{\\sum\\limits_{i=1}^k\\lambda_i}{\\sum\\limits_{i=1}^n\\lambda_i}\\geq \\alpha.\n",
    "$$\n",
    "\n",
    "## PCA over the features space for dimensionality reduction in the linear regression\n",
    "\n",
    "The leading principle components contain most of the information from the entire original feature space, and might be suitable to represent the feature space reducing its dimensionality. This however does not guarantee (although usually provides a good chance) that those leading components are indeed the ones most relevant for modeling the output variables, so reducing dimensionality of the regression model by selecting the leading principle components does not always lead to the best outcome. \n",
    "\n",
    "In this case, using regular feature selection (e.g. backward step-wise or forward step-wise) could be the best. PCA however will help by ensuring that the features are not interrelated and can be effectively excluded or included one-by-one. This way feature selection after PCA is always more efficient than the feature selection over the original non-transformed feature space. \n",
    "\n",
    "This also makes it more reasible (although still not entirely) to rely on the \"naive\" feature selection based on p-values. While p-values still represent an intrinsic property of the training set, and this way can not guarantee generalizeability, since the standardized principle components are non-correlated/othogonal, a low p-value is a good indicator of the regressor's utility as long as the size/choice of the training data is sufficient to exclude random coincidences.\n",
    "\n",
    "The most significant downside of using PCA for feature selection is that the remaining selected principle components could be pretty hard to interpret as they often look like arbitrary linear combinations of multiple features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting a linear regression with multiple features $x=(x^1,x^2,...,x^n)$ \n",
    "$$\n",
    "y\\sim w^T x\n",
    "$$\n",
    "the model might become complex and susceptible to overfitting. Complexity often comes with coefficients $w$ growing large in absolute values.\n",
    "\n",
    "One way to reduce complexity is to penalize regression for the magnitude of the coefficients $w=(w^1,w^2,...,w^n)$, which can be measured by\n",
    "$$\n",
    "||w||_1=\\sum\\limits_j |w^j|\n",
    "$$\n",
    "or by \n",
    "$$\n",
    "||w||_2^2=\\sum\\limits_j (w^j)^2\n",
    "$$\n",
    "So instead of simply minimizing \n",
    "$$\n",
    "RSS(w)=\\sum\\limits_j (y_j-w^T x_j)^2\n",
    "$$\n",
    "where $x_j, y_j$ are observations for regressors and output variable, we minimize\n",
    "$$\n",
    "RSS(w)+\\lambda ||w||_2^2=RSS(w)+\\lambda \\sum\\limits_j (w^j)^2\\to\\min\\hspace{10ex}(Ridge)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "RSS(w)+\\lambda ||w||_1=RSS(w)+\\lambda \\sum\\limits_j \\left|w^j\\right|\\to\\min\\hspace{10ex}(Lasso)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model is known as Ridge, the second - as Lasso (least absolute shrinkage and selection operator) regularized regression.\n",
    "\n",
    "Both Ridge and Lasso could be shown to be equivalent to a constrained minimization of $RSS$:\n",
    "\n",
    "$$\n",
    "RSS(w)\\to min, \\ ||w||_p\\leq \\alpha,\n",
    "$$\n",
    "\n",
    "with $p=1,2$ respectively, although analytic relation between constants $\\alpha$ and $\\lambda$ is somewhat nontrivial. In practice however the choice of $\\lambda$ or $\\alpha$ is usually empirical anyway, so both regularized or constrained forms of the optimization problem are equally applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization problems (Ridge) and (Lasso) tend to minimize $RSS$ at the same time penalizing the regression for having $||w||$ too large (regularization) which often leads to the model complexity through multiple regressors with large coefficients canceling effect of each other. So in a sense Lasso and Ridge are trying to avoid this situation, looking for relatively simple \"regular\" models with best possible fit. \n",
    "\n",
    "** NOTICE ** As the order of magnitude of $w^j$ is directly related to the scale of the regressors, it is practical to rescale them (e.g. by standardizing) to make sure the $w^j$ are comparable in scale. Otherwise penalization terms directly mixing components $w^j$ of different, sometimes incomparable, scale do not make too much sense.\n",
    "\n",
    "From Bayesian standpoint (for those familiar with Bayesian inference) Lasso and Ridge simply perform the regression with the prior belief that all the components of the $w$ are limited through the fixed variance of the priors. Such a belief affects the final outcome of the model making solutions with large $||w||$ to be particularly unlikely.\n",
    "\n",
    "This helps Ridge and Lasso to fight overfitting also dealing with multicollinearity of regressors to some extent, preventing from learning noise through particularly complex \"unnatural\" combinations of the regressors.\n",
    "\n",
    "Ridge regression admits solution in the closed form (consider partial derivatives of the objective function with respect to $w_j$):\n",
    "\n",
    "$$\n",
    "\\hat{w}=(X^T X+\\lambda I)^{-1}X^T Y, \\hspace{5ex}(Ridge\\ solution)\n",
    "$$\n",
    "\n",
    "where $I$ is the identity $n\\times n$ matrix, while $n$ being the number of regressors. The formulae (Ridge solution) shows that the Ridge regression can in theory deal with the case of multicollinearity, when the matrix $X^T X$ is singular and OLS estimate does not exist.\n",
    "\n",
    "Lasso does not admit solution in the closed form and requires numerical methods (like subgradient methods) to be fit. \n",
    "Lasso however has an advantage of being often able to completely eliminate impact of certain irrelevant regressors setting the corresponding slope coefficients to zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Clustering\n",
    "## K-means clustering\n",
    "\n",
    "What if we do not have any training data? Unsupervised approach - k-means clustering can help learning the most suitable cluster structure given a suggested number of clusters $k$. \n",
    "\n",
    "Works for the data samples ($N$ points) in $n$-dimensional Euclidean space: \n",
    "$$\n",
    "X=\\{x_i, i=1..N\\}=\\{x_i^j, i=1..N, j=1..n\\},\n",
    "$$ \n",
    "where $x_i$ are the $n$-dimensional raw-vectors corresponding to each of the $N$ sample points. The approach is trying to infer cluster indexes $c_i$ (taking one of the possible values $1,2,...,k$) for each point $x_i$ in such a way that the cumulative squared distance from all the sample points $x_i$ to the centroids $\\mu_{c_i}$ of the corresponding clusters $c_i$ is minimized. This is quite an intuitive criteria, meaning that the clusters as as compact as possible for the given number $k$. The cluster centroids are defined as simple vector means\n",
    "$$\n",
    "\\mu_c=\\sum_{i, c(i)=c} x_i/m(c),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "m(c)=|\\{i, c(i)=c\\}|\n",
    "$$\n",
    "denotes the cluster size. Or in the coordinate form $\\mu_c=(\\mu_c^j, j=1..n)$\n",
    "$$\n",
    "\\mu_c^j=\\frac{\\sum_{i, c(i)=c} x_i^j}{m(c)}, j=1..n.\n",
    "$$\n",
    "Then the mimimal cumulatize squared distance criteria can be written as\n",
    "$$\n",
    "SD=\\sum_i \\|x_i-\\mu_{c_i}\\|^2=\\sum_{i,j} \\left(x_i^j-\\mu_{c_i}^j\\right)^2\\to \\min\n",
    "$$\n",
    "\n",
    "The common algorithm (often called after Lloyd) starts from a random cluster assignment and iterates the following two steps:\n",
    "\n",
    "A. Compute cluster means.\n",
    "\n",
    "B. Re-assign each point to the cluster with the mean closest to the considered point.\n",
    "\n",
    "Alternative approach is to initially use $k$ random points of the sample as clusters means, starting from the step B.\n",
    "\n",
    "The algorithm stops once a new iteration fails to alter any single cluster assignment.\n",
    "\n",
    "Algorithm always converges to a local optimum of $SD$, however there is no guarantee that this partitioning is indeed the global optimum. Also the final outcome happens to depend on the initial partitioning. This way it makes sense to repreat $k$-means a number of times (like $10,25...100$ etc) with different random initial partitioning, picking up the partitioning with the best final $SD$. \n",
    "\n",
    "In order to figure out how many trials to consider one can keep adding them one by one tracking how do they affect the final score and stop once it stays stable for a while.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Choose the number of clusters. (k)\n",
    "\n",
    "#### Silhouette Coefficient\n",
    "In the example above we visually see that the data could still be clustered further. So how to select the appropriate number of clusters $k$ besides just naive visual observations? \n",
    "\n",
    "We can't use SD here anymore as the more clusters we take, the smaller it goes, and we'll get it down to zero, assigning each point to its own cluster, which is certianly not the most useful way of clustering.\n",
    "\n",
    "We need another partitioning quality measure here. Most common option is using a Silhouette measure, which for each data point $x_i$ quantifies its relative attachement strength to its current cluster vs the closest neighbor cluster:\n",
    "$$\n",
    "s(i)=\\frac{\\min\\limits_{k\\neq c_i} \\|x_i-\\mu_{c_k}\\|-\\|x_i-\\mu_{c_i}\\|}{\\max\\{\\|x_i-\\mu_{c_i}\\|,\\min\\limits_{k\\neq c_i} \\|x_i-\\mu_{c_k}\\|\\}}.\n",
    "$$\n",
    "\n",
    "Then internal quality of the partitioning is characterized by an average ratio value of silhouette for all the data points:\n",
    "$$\n",
    "S=\\frac{\\sum\\limits_i s(i)}{N}.\n",
    "$$\n",
    "\n",
    "By definition, the silhouette measure is normalized. i.e. $-1\\leq s(i)\\leq 1$ and so is the average: $-1\\leq S\\leq 1$. $S=1$ means that all the points coinside with their cluster centroids, values close to 1 mean that the points are usually much closer to their cluster centroids than to all the other ones (i.e. clustered appropriately), values of $S$ around zero mean that clustering is quite unstable, i.e. moving many of the points to another clusters might not affect its quality much; while negative values of $S$ mean that clustering could certainly be improved by moving some of the points to the other clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method\n",
    "The Elbow method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified.\n",
    "\n",
    "First of all, compute the sum of squared error (SSE) for some values of k (for example 2, 4, 6, 8, etc.). The SSE is defined as the sum of the squared distance between each member of the cluster and its centroid. Mathematically:\n",
    "\n",
    "$SSE=\\sum^K_{i=1} \\sum_{x\\in c_i} dist(x,c_i)^2$\n",
    "\n",
    " If you plot k against the SSE, you will see that the error decreases as k gets larger; this is because when the number of clusters increases, they should be smaller, so distortion is also smaller. The idea of the elbow method is to choose the k at which the SSE decreases abruptly. This produces an \"elbow effect\" in the graph, as you can see in the following picture:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
